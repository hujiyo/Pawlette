import os
import sys

import time
import math
import warnings
import torch
import torch.distributed as dist
from torch import optim, nn
from torch.nn.parallel import DistributedDataParallel
from torch.utils.data import DataLoader, DistributedSampler
from transformers import AutoTokenizer
from model.model_pawlette import PawletteConfig, PawletteModelLLM, count_parameters
from dataset.lm_dataset import PretrainDataset, dynamic_collate_fn

warnings.filterwarnings('ignore')

# è®­ç»ƒç›¸å…³å‚æ•°é…ç½®
CONFIG = {
    # è®­ç»ƒé…ç½®
    'epochs': 1,
    'batch_size': 32,  
    'learning_rate': 5e-4,
    'warmup_steps': 104, #æŒ‡é¢„çƒ­æ­¥æ•°
    'accumulation_steps': 8,  # å¢åŠ æ¢¯åº¦ç´¯ç§¯æ­¥æ•°ä»¥ä¿æŒæœ‰æ•ˆæ‰¹æ¬¡å¤§å°
    'grad_clip': 1.0,
    'weight_decay': 0.01,
    
    # æ•°æ®é…ç½®
    'data_path': 'dataset/pretrain_data.jsonl',
    'max_seq_len': None,  # ä¸é™åˆ¶åºåˆ—é•¿åº¦
    'tokenizer_path': 'model/',

    # è¾“å‡ºé…ç½®
    'out_dir': 'out',
    'log_interval': 80,
    'save_interval': 800,
    
    # ç»§ç»­è®­ç»ƒé…ç½®
    'continue_pretrain': False,
    'pretrained_path': None,
    'resume': False,
    'checkpoint_path': None,
    
    # åˆ†å¸ƒå¼è®­ç»ƒ
    'ddp': False,
    'num_workers': 4,
    
    # å…¶ä»–é…ç½®
    'device': 'cuda:0',  # Pawletteå¿…é¡»ä½¿ç”¨GPUè®­ç»ƒï¼ˆMamba2ä¾èµ–CUDAï¼‰
    'seed': 42,
    'use_wandb': False,
    'wandb_project': 'Pawlette-Pretrain',
}

def Logger(content):
    """ç»Ÿä¸€çš„æ—¥å¿—è¾“å‡ºå‡½æ•°"""
    try:
        if not CONFIG['ddp'] or dist.get_rank() == 0:
            print(f"[Pawlette] {content}")
    except NameError:
        print(f"[Pawlette] {content}")

def get_cosine_schedule_with_warmup(current_step, num_warmup_steps, num_training_steps, min_lr_ratio=0.1):
    """ä½™å¼¦é€€ç«å­¦ä¹ ç‡è°ƒåº¦ï¼ˆå¸¦warmupï¼‰"""
    if current_step < num_warmup_steps:
        # Warmupé˜¶æ®µ
        return float(current_step) / float(max(1, num_warmup_steps))  
    # ä½™å¼¦é€€ç«é˜¶æ®µ
    progress = float(current_step - num_warmup_steps) / float(max(1, num_training_steps - num_warmup_steps))
    lr_mult = min_lr_ratio + (1 - min_lr_ratio) * 0.5 * (1.0 + math.cos(math.pi * progress))
    return lr_mult

def save_checkpoint(epoch, step, model, optimizer, scaler, save_path, global_step=None):
    """ä¿å­˜æ£€æŸ¥ç‚¹"""
    state = {
        'epoch': epoch,
        'step': step,
        'global_step': global_step,  # æ–°å¢ï¼šä¿å­˜å…¨å±€æ­¥æ•°
        'model_state_dict': model.module.state_dict() if isinstance(model, DistributedDataParallel) else model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'scaler_state_dict': scaler.state_dict(),
        'config': model.module.config if isinstance(model, DistributedDataParallel) else model.config,
    }
    torch.save(state, save_path)
    Logger(f"âœ… å·²ä¿å­˜æ£€æŸ¥ç‚¹è‡³ {save_path}")

def load_checkpoint(model, optimizer, scaler, checkpoint_path, device, strict=True):
    """åŠ è½½æ£€æŸ¥ç‚¹"""
    checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=False)
    # åŠ è½½æ¨¡å‹çŠ¶æ€
    if isinstance(model, DistributedDataParallel):
        model.module.load_state_dict(checkpoint['model_state_dict'], strict=strict)
    else:
        model.load_state_dict(checkpoint['model_state_dict'], strict=strict)  
    # åŠ è½½ä¼˜åŒ–å™¨å’ŒscalerçŠ¶æ€
    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
    scaler.load_state_dict(checkpoint['scaler_state_dict'])
    
    start_epoch = checkpoint.get('epoch', 0)
    start_step = checkpoint.get('step', 0)
    start_global_step = checkpoint.get('global_step', 0)  # æ–°å¢ï¼šåŠ è½½å…¨å±€æ­¥æ•°
    Logger(f"âœ… å·²ä» {checkpoint_path} åŠ è½½æ£€æŸ¥ç‚¹")
    Logger(f"   ç»§ç»­è®­ç»ƒ: epoch={start_epoch}, step={start_step}, global_step={start_global_step}")
    
    return start_epoch, start_step, start_global_step

def train_epoch(epoch, start_step, model, train_loader, optimizer, scaler, 
                scheduler_fn, ctx, wandb=None, start_epoch=0, start_global_step=0):
    """è®­ç»ƒä¸€ä¸ªepoch"""
    model.train()    
    total_loss = 0
    start_time = time.time()
    first_step = True  # æ ‡è®°æ˜¯å¦æ˜¯ç¬¬ä¸€ä¸ªå®é™…æ‰§è¡Œçš„æ­¥éª¤
    executed_steps = 0  # å®é™…æ‰§è¡Œçš„æ­¥æ•°è®¡æ•°å™¨
    accumulation_counter = 0  # æ¢¯åº¦ç´¯ç§¯è®¡æ•°å™¨
    
    # ä½¿ç”¨checkpoint/ä¸Šå±‚ä¼ å…¥çš„å…¨å±€æ­¥æ•°ä½œä¸ºå”¯ä¸€çœŸå€¼
    current_global_step = start_global_step
    
    for step, (input_ids, labels, loss_mask) in enumerate(train_loader):
        # è·³è¿‡å·²è®­ç»ƒçš„æ­¥éª¤ï¼ˆç”¨äºæ–­ç‚¹ç»­è®­ï¼‰
        if epoch == start_epoch and step < start_step:
            # è·³è¿‡æ—¶ä¸é€’å¢global_stepï¼›global_stepä»…è®¡æ•°â€œå®é™…æ‰§è¡Œè¿‡â€çš„batch
            continue
        
        # å¦‚æœæ˜¯ç¬¬ä¸€ä¸ªå®é™…æ‰§è¡Œçš„æ­¥éª¤ï¼Œé‡æ–°è®¾ç½®å¼€å§‹æ—¶é—´
        if first_step:
            start_time = time.time()
            first_step = False
        
        # å¢åŠ å®é™…æ‰§è¡Œçš„æ­¥æ•°è®¡æ•°
        executed_steps += 1
        accumulation_counter += 1
        
        input_ids = input_ids.to(CONFIG['device'])
        labels = labels.to(CONFIG['device'])
        attention_mask = (input_ids != 6).long()  # pad_token_id = 6
        
        #ä½¿ç”¨æ­£ç¡®çš„å…¨å±€æ­¥æ•°
        global_step = current_global_step
        
        # å‰å‘ä¼ æ’­ - ä½¿ç”¨LongSSMéšè—çŠ¶æ€å¤ç”¨æœºåˆ¶
        # åœ¨batchå†…,ç¬¬ä¸€ä¸ªåºåˆ—é›¶åˆå§‹åŒ–,åç»­åºåˆ—å¤ç”¨å‰ä¸€ä¸ªåºåˆ—çš„éšè—çŠ¶æ€ä»¥æ”¹å–„é•¿åº¦å¤–æ¨
        batch_size = input_ids.size(0)
        all_losses = []
        
        with ctx:
            # åˆå§‹åŒ–inference_paramsç”¨äºå­˜å‚¨éšè—çŠ¶æ€
            from mamba_ssm.utils.generation import InferenceParams
            inference_params = None
            
            for batch_idx in range(batch_size):
                # è·å–å½“å‰åºåˆ—
                current_input_ids = input_ids[batch_idx:batch_idx+1]  # [1, seq_len]
                current_labels = labels[batch_idx:batch_idx+1]  # [1, seq_len]
                current_attention_mask = attention_mask[batch_idx:batch_idx+1] if attention_mask is not None else None
                
                # ç¬¬ä¸€ä¸ªåºåˆ—ä½¿ç”¨é›¶åˆå§‹åŒ–(inference_params=None),åç»­åºåˆ—å¤ç”¨éšè—çŠ¶æ€
                if batch_idx == 0:
                    inference_params = None  # ç¬¬ä¸€ä¸ªåºåˆ—é›¶åˆå§‹åŒ–
                # else: å¤ç”¨ä¸Šä¸€ä¸ªåºåˆ—çš„inference_params
                
                # å‰å‘ä¼ æ’­
                outputs = model(
                    input_ids=current_input_ids,
                    labels=current_labels,
                    attention_mask=current_attention_mask,
                    inference_params=inference_params,
                    use_cache=True  # å¯ç”¨ç¼“å­˜ä»¥ä¿å­˜éšè—çŠ¶æ€
                )
                
                all_losses.append(outputs.loss)
                
                # æ›´æ–°inference_paramsä¾›ä¸‹ä¸€ä¸ªåºåˆ—ä½¿ç”¨
                inference_params = outputs.past_key_values
                if inference_params is not None:
                    # é‡ç½®seqlen_offset,è®©ä¸‹ä¸€ä¸ªåºåˆ—ä»å¤´å¼€å§‹ä½†å¤ç”¨éšè—çŠ¶æ€
                    inference_params.seqlen_offset = 0
            
            # è®¡ç®—batchçš„å¹³å‡æŸå¤±
            loss = torch.stack(all_losses).mean()
            loss = loss / CONFIG['accumulation_steps']# æ¢¯åº¦ç´¯ç§¯
        # åå‘ä¼ æ’­
        scaler.scale(loss).backward()
        
        # æ›´æ–°å­¦ä¹ ç‡ï¼ˆæ¯ä¸ªbatchéƒ½æ›´æ–°ï¼‰
        # ä½¿ç”¨å®é™…å­¦ä¹ ç‡ï¼ˆå·²ç»æ ¹æ®é¢„è®­ç»ƒæ¨¡å‹è°ƒæ•´è¿‡ï¼‰
        lr_mult = scheduler_fn(global_step)
        for param_group in optimizer.param_groups:
            param_group['lr'] = CONFIG['actual_learning_rate'] * lr_mult
        
        # æ¢¯åº¦ç´¯ç§¯æ­¥éª¤ - ä½¿ç”¨ç‹¬ç«‹è®¡æ•°å™¨é¿å…æ–­ç‚¹ç»­è®­æ—¶é”™ä½
        if accumulation_counter % CONFIG['accumulation_steps'] == 0:
            # æ¢¯åº¦è£å‰ª
            scaler.unscale_(optimizer)
            torch.nn.utils.clip_grad_norm_(model.parameters(), CONFIG['grad_clip'])
            
            # ä¼˜åŒ–å™¨æ­¥éª¤
            scaler.step(optimizer)
            scaler.update()
            optimizer.zero_grad(set_to_none=True)
        
        # æ¯ä¸ªbatchåéƒ½è¦é€’å¢global_step
        current_global_step += 1
        # ç»Ÿè®¡
        total_loss += loss.item() * CONFIG['accumulation_steps']
        # æ—¥å¿—è¾“å‡º
        if step % CONFIG['log_interval'] == 0:
            elapsed_time = time.time() - start_time            
            if executed_steps > 0:
                avg_time_per_step = elapsed_time / executed_steps
                # è®¡ç®—å‰©ä½™æ­¥æ•°
                remaining_steps = len(train_loader) - step - 1
                remaining_time = avg_time_per_step * remaining_steps / 60
            else:
                remaining_time = 0
            
            current_loss = loss.item() * CONFIG['accumulation_steps']
            current_lr = optimizer.param_groups[0]['lr']            
            Logger(
                f'Epoch:[{epoch+1}/{CONFIG["epochs"]}]({step}/{len(train_loader)}) '
                f'loss:{current_loss:.4f} lr:{current_lr:.2e} '
                f'eta:{remaining_time:.1f}min'
            )            
            # WandBæ—¥å¿—
            if wandb is not None and (not CONFIG['ddp'] or dist.get_rank() == 0):
                wandb.log({
                    "train/loss": current_loss,
                    "train/lr": current_lr,
                    "train/global_step": global_step,
                })
        
        # å®šæœŸä¿å­˜æ£€æŸ¥ç‚¹
        if (step + 1) % CONFIG['save_interval'] == 0 and (not CONFIG['ddp'] or dist.get_rank() == 0):
            checkpoint_path = os.path.join(CONFIG['save_dir'], 'checkpoint_latest.pth')
            save_checkpoint(epoch, step + 1, model, optimizer, scaler, checkpoint_path, current_global_step)
            
            # ä¿å­˜æ¨¡å‹æƒé‡
            model_path = os.path.join(CONFIG['save_dir'], 'pawlette.pth')
            if isinstance(model, DistributedDataParallel):
                torch.save(model.module.state_dict(), model_path)
            else:
                torch.save(model.state_dict(), model_path)
            Logger(f"âœ… å·²ä¿å­˜æ¨¡å‹æƒé‡è‡³ {model_path}")
    # ä½¿ç”¨å®é™…æ‰§è¡Œçš„æ­¥æ•°è®¡ç®—å¹³å‡æŸå¤±
    avg_loss = total_loss / max(1, executed_steps)
    return avg_loss, current_global_step  #è¿”å›æœ€æ–°çš„global_step

def init_model():
    """åˆå§‹åŒ–æ¨¡å‹å’Œåˆ†è¯å™¨"""
    config = PawletteConfig()
    Logger(f"ğŸ“ æ¨¡å‹é…ç½®:")
    Logger(f"   - hidden_size: {config.hidden_size}")
    Logger(f"   - num_layers: {config.num_hidden_layers}")
    Logger(f"   - state_size: {config.state_size}")
    Logger(f"   - vocab_size: {config.vocab_size}")

    model = PawletteModelLLM(config)
    
    # åŠ è½½é¢„è®­ç»ƒæƒé‡ï¼ˆå¦‚æœæŒ‡å®šï¼‰
    loaded_pretrained = False  # æ ‡è®°æ˜¯å¦åŠ è½½äº†é¢„è®­ç»ƒæ¨¡å‹
    if CONFIG['continue_pretrain'] and CONFIG['pretrained_path']:
        if os.path.exists(CONFIG['pretrained_path']):
            Logger(f"ğŸ“‚ åŠ è½½é¢„è®­ç»ƒæ¨¡å‹: {CONFIG['pretrained_path']}")
            state_dict = torch.load(CONFIG['pretrained_path'], map_location=CONFIG['device'])
            model.load_state_dict(state_dict, strict=False)
            Logger("âœ… é¢„è®­ç»ƒæ¨¡å‹åŠ è½½æˆåŠŸ")
            loaded_pretrained = True
        else:
            Logger(f"âš ï¸ é¢„è®­ç»ƒæ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {CONFIG['pretrained_path']}")
    
    # åŠ è½½åˆ†è¯å™¨
    tokenizer_path = CONFIG['tokenizer_path']
    tokenizer = AutoTokenizer.from_pretrained(tokenizer_path, use_fast=True)
    config.pad_token_id = tokenizer.pad_token_id
    config.bos_token_id = tokenizer.bos_token_id
    config.eos_token_id = tokenizer.eos_token_id

    # ç§»åŠ¨æ¨¡å‹åˆ°è®¾å¤‡
    model = model.to(CONFIG['device'])
    # ç»Ÿè®¡å‚æ•°é‡
    params = count_parameters(model)
    Logger(f"ğŸ“Š æ¨¡å‹å‚æ•°é‡: {params['trainable_M']:.2f}M (å¯è®­ç»ƒ) / {params['total_M']:.2f}M (æ€»è®¡)")
    return model, tokenizer, config, loaded_pretrained

def init_distributed_mode():
    """åˆå§‹åŒ–åˆ†å¸ƒå¼è®­ç»ƒ"""
    if not CONFIG['ddp']:
        return
    dist.init_process_group(backend="nccl")
    CONFIG['ddp_rank'] = int(os.environ["RANK"])
    CONFIG['ddp_local_rank'] = int(os.environ["LOCAL_RANK"])
    CONFIG['ddp_world_size'] = int(os.environ["WORLD_SIZE"])
    CONFIG['device'] = f"cuda:{CONFIG['ddp_local_rank']}"
    torch.cuda.set_device(CONFIG['device'])
    Logger(f"ğŸŒ åˆ†å¸ƒå¼è®­ç»ƒåˆå§‹åŒ–: rank={CONFIG['ddp_rank']}, world_size={CONFIG['ddp_world_size']}")

def main():
    """
    Pawletteæ¨¡å‹é¢„è®­ç»ƒä¸»å‡½æ•°
    åªæ”¯æŒä½¿ç”¨GPUè®­ç»ƒï¼ˆMamba2ä¾èµ–CUDAï¼‰
    """
    # æ£€æŸ¥CUDAå¯ç”¨æ€§
    if not torch.cuda.is_available():
        raise RuntimeError(
            "âŒ Pawletteéœ€è¦GPUæ”¯æŒï¼\n"
            "Mamba2æ¶æ„ä¾èµ–CUDA kernelï¼Œæ— æ³•åœ¨CPUä¸Šè®­ç»ƒã€‚\n"
        )
    Logger(f"ğŸ® æ£€æµ‹åˆ°GPU: {torch.cuda.get_device_name(0)}")
    
    # è®¾ç½®éšæœºç§å­
    torch.manual_seed(CONFIG['seed'])
    torch.cuda.manual_seed(CONFIG['seed'])
    
    # åˆå§‹åŒ–åˆ†å¸ƒå¼è®­ç»ƒ
    if CONFIG['ddp']:
        init_distributed_mode()
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    CONFIG['save_dir'] = os.path.join(CONFIG['out_dir'], "pawlette")
    os.makedirs(CONFIG['save_dir'], exist_ok=True)
    
    # åˆå§‹åŒ–WandB
    wandb = None
    if CONFIG['use_wandb'] and (not CONFIG['ddp'] or dist.get_rank() == 0):
        import wandb
        run_name = f"Pawlette-bs{CONFIG['batch_size']}-lr{CONFIG['learning_rate']}"
        wandb.init(project=CONFIG['wandb_project'], name=run_name, config=CONFIG)
    
    Logger("ğŸ¾ Pawletteé¢„è®­ç»ƒå¼€å§‹")
    Logger(f"ğŸ“ è¾“å‡ºç›®å½•: {CONFIG['save_dir']}")
    
    # è®¾ç½®æ··åˆç²¾åº¦ï¼ˆå›ºå®šä½¿ç”¨bfloat16ï¼‰
    ctx = torch.cuda.amp.autocast(dtype=torch.bfloat16)
    Logger(f"ğŸ”¢ è®­ç»ƒç²¾åº¦: bfloat16 (æ··åˆç²¾åº¦è®­ç»ƒ)")
    
    # åˆå§‹åŒ–æ¨¡å‹å’Œåˆ†è¯å™¨
    model, tokenizer, config, loaded_pretrained = init_model()
    
    # å‡†å¤‡æ•°æ®é›†
    train_dataset = PretrainDataset(CONFIG['data_path'], tokenizer, max_length=CONFIG['max_seq_len'])
    Logger(f"ğŸ“š è®­ç»ƒæ•°æ®é›†å¤§å°: {len(train_dataset)}")
    if CONFIG['max_seq_len'] is None:
        Logger("ğŸ“ åºåˆ—é•¿åº¦: æ— é™åˆ¶ï¼ˆåŠ¨æ€é•¿åº¦ï¼‰")
    else:
        Logger(f"ğŸ“ åºåˆ—é•¿åº¦: {CONFIG['max_seq_len']}")
    
    # æ•°æ®åŠ è½½å™¨
    train_sampler = DistributedSampler(train_dataset) if CONFIG['ddp'] else None
    train_loader = DataLoader(
        train_dataset,
        batch_size=CONFIG['batch_size'],
        shuffle=(train_sampler is None),
        sampler=train_sampler,
        num_workers=CONFIG['num_workers'],
        pin_memory=True,
        drop_last=True,
        collate_fn=dynamic_collate_fn if CONFIG['max_seq_len'] is None else None,
    )
    
    # ä¼˜åŒ–å™¨ - æ ¹æ®æ˜¯å¦ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹è°ƒæ•´å­¦ä¹ ç‡
    actual_lr = CONFIG['learning_rate']
    if loaded_pretrained:
        actual_lr = CONFIG['learning_rate'] * 0.5  # ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹æ—¶ï¼Œå­¦ä¹ ç‡é™ä½åˆ°50%
        Logger(f"ğŸ”§ æ£€æµ‹åˆ°ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹åˆå§‹åŒ–ï¼Œå­¦ä¹ ç‡è°ƒæ•´ä¸º: {actual_lr:.2e} (åŸå­¦ä¹ ç‡çš„50%)")
    
    # ä¿å­˜å®é™…å­¦ä¹ ç‡åˆ°CONFIGï¼Œä¾›å­¦ä¹ ç‡è°ƒåº¦å™¨ä½¿ç”¨
    CONFIG['actual_learning_rate'] = actual_lr
    
    optimizer = optim.AdamW(
        model.parameters(),
        lr=actual_lr,
        weight_decay=CONFIG['weight_decay'],
        betas=(0.9, 0.95),
    )
    
    # æ··åˆç²¾åº¦è®­ç»ƒï¼ˆbfloat16 + CUDAï¼‰
    scaler = torch.cuda.amp.GradScaler()
    
    # å­¦ä¹ ç‡è°ƒåº¦å™¨ - åŸºäºbatchæ­¥æ•°ï¼ˆä¸oldç‰ˆæœ¬ä¸€è‡´ï¼‰
    total_steps = len(train_loader) * CONFIG['epochs']
    
    scheduler_fn = lambda step: get_cosine_schedule_with_warmup(
        step, CONFIG['warmup_steps'], total_steps, min_lr_ratio=0.1
    )
    
    # æ–­ç‚¹ç»­è®­ - è‡ªåŠ¨æ£€æµ‹æ£€æŸ¥ç‚¹æ–‡ä»¶
    start_epoch, start_step, start_global_step = 0, 0, 0
    
    # è‡ªåŠ¨æ£€æµ‹æ£€æŸ¥ç‚¹æ–‡ä»¶
    checkpoint_path = os.path.join(CONFIG['save_dir'], 'checkpoint_latest.pth')
    if os.path.exists(checkpoint_path):
        Logger(f"ğŸ” è‡ªåŠ¨æ£€æµ‹åˆ°æ£€æŸ¥ç‚¹æ–‡ä»¶: {checkpoint_path}")
        Logger("ğŸ”„ è‡ªåŠ¨å¯ç”¨æ–­ç‚¹ç»­è®­...")
        start_epoch, start_step, start_global_step = load_checkpoint(
            model, optimizer, scaler, checkpoint_path, CONFIG['device']
        )
    elif CONFIG['resume'] and CONFIG['checkpoint_path']:
        # å¦‚æœæ‰‹åŠ¨æŒ‡å®šäº†æ£€æŸ¥ç‚¹è·¯å¾„
        checkpoint_path = CONFIG['checkpoint_path']
        if os.path.exists(checkpoint_path):
            start_epoch, start_step, start_global_step = load_checkpoint(
                model, optimizer, scaler, checkpoint_path, CONFIG['device']
            )
        else:
            Logger(f"âš ï¸ æœªæ‰¾åˆ°æ£€æŸ¥ç‚¹æ–‡ä»¶: {checkpoint_path}")
    else:
        # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨è®­ç»ƒå¥½çš„æ¨¡å‹æ–‡ä»¶
        existing_models = []
        model_files = [
            'pawlette.pth',
        ]
        
        for model_file in model_files:
            model_path = os.path.join(CONFIG['save_dir'], model_file)
            if os.path.exists(model_path):
                existing_models.append(model_file)
        
        if existing_models:
            Logger("âš ï¸ æ£€æµ‹åˆ°å·²å­˜åœ¨çš„æ¨¡å‹æ–‡ä»¶:")
            for model_file in existing_models:
                Logger(f"   - {model_file}")
            Logger("")
            Logger("è¯·é€‰æ‹©æ“ä½œ:")
            Logger("  1. åŠ è½½ pawlette.pth ä½œä¸ºåˆå§‹åŒ–å‚æ•°ï¼Œä»å¤´å¼€å§‹è®­ç»ƒ")
            Logger("  2. ç»ˆæ­¢è®­ç»ƒï¼ˆé˜²æ­¢è¦†ç›–ï¼‰")
            Logger("")
            
            # åªåœ¨ä¸»è¿›ç¨‹è¯¢é—®ç”¨æˆ·
            if not CONFIG['ddp'] or dist.get_rank() == 0:
                try:
                    choice = input("è¯·è¾“å…¥é€‰æ‹© (1/2): ").strip()
                    
                    if choice == "1":
                        Logger("âœ… ç”¨æˆ·é€‰æ‹©ï¼šåŠ è½½ pawlette.pth ä½œä¸ºåˆå§‹åŒ–å‚æ•°")
                        model_path = os.path.join(CONFIG['save_dir'], 'pawlette.pth')
                        state_dict = torch.load(model_path, map_location=CONFIG['device'])
                        # æ­¤æ—¶modelè¿˜æœªè¢«DDPåŒ…è£…ï¼Œç›´æ¥åŠ è½½å³å¯
                        model.load_state_dict(state_dict, strict=False)
                        Logger(f"ğŸ“‚ å·²åŠ è½½æ¨¡å‹æƒé‡: {model_path}")
                        Logger("ğŸ†• å¼€å§‹ä»å¤´è®­ç»ƒï¼ˆä½¿ç”¨å·²æœ‰æ¨¡å‹ä½œä¸ºåˆå§‹åŒ–ï¼‰...")
                        
                        # é‡æ–°è°ƒæ•´å­¦ä¹ ç‡ï¼ˆå› ä¸ºæ­¤æ—¶ä¼˜åŒ–å™¨å·²ç»åˆ›å»ºï¼‰
                        actual_lr = CONFIG['learning_rate'] * 0.5
                        CONFIG['actual_learning_rate'] = actual_lr
                        for param_group in optimizer.param_groups:
                            param_group['lr'] = actual_lr
                        Logger(f"ğŸ”§ æ£€æµ‹åˆ°ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹åˆå§‹åŒ–ï¼Œå­¦ä¹ ç‡è°ƒæ•´ä¸º: {actual_lr:.2e} (åŸå­¦ä¹ ç‡çš„50%)")
                    elif choice == "2":
                        Logger("ğŸ›‘ ç”¨æˆ·é€‰æ‹©ï¼šç»ˆæ­¢è®­ç»ƒ")
                        Logger("ğŸ’¡ å¦‚éœ€ç»§ç»­è®­ç»ƒï¼Œè¯·:")
                        Logger("   1. åˆ é™¤æˆ–é‡å‘½åç°æœ‰æ¨¡å‹æ–‡ä»¶")
                        Logger("   2. æˆ–è€…ä½¿ç”¨ --resume å‚æ•°æŒ‡å®šæ£€æŸ¥ç‚¹æ–‡ä»¶")
                        Logger("   3. æˆ–è€…ä¿®æ”¹è¾“å‡ºç›®å½• --out_dir")
                        return
                    else:
                        Logger("âŒ æ— æ•ˆçš„é€‰æ‹©ï¼Œè®­ç»ƒå·²ç»ˆæ­¢")
                        return
                except (KeyboardInterrupt, EOFError):
                    Logger("\nğŸ›‘ ç”¨æˆ·å–æ¶ˆæ“ä½œï¼Œè®­ç»ƒå·²ç»ˆæ­¢")
                    return
            else:
                # éä¸»è¿›ç¨‹ç­‰å¾…ä¸»è¿›ç¨‹çš„å†³å®š
                # è¿™é‡Œå¯ä»¥é€šè¿‡åˆ†å¸ƒå¼é€šä¿¡åŒæ­¥å†³å®šï¼Œä½†ç®€åŒ–å¤„ç†
                pass
        else:
            Logger("ğŸ†• æœªæ£€æµ‹åˆ°æ£€æŸ¥ç‚¹æ–‡ä»¶å’Œæ¨¡å‹æ–‡ä»¶ï¼Œå¼€å§‹å…¨æ–°è®­ç»ƒ...")
    
    # DDPåŒ…è£…
    if CONFIG['ddp']:
        model = DistributedDataParallel(model, device_ids=[CONFIG['ddp_local_rank']])
    
    #åˆå§‹åŒ–å½“å‰çš„å…¨å±€æ­¥æ•°è·Ÿè¸ª
    current_global_step = start_global_step
    
    # è®­ç»ƒå¾ªç¯
    Logger("ğŸš€ å¼€å§‹è®­ç»ƒ...")
    for epoch in range(start_epoch, CONFIG['epochs']):
        if CONFIG['ddp'] and train_sampler is not None:
            train_sampler.set_epoch(epoch)
        
        # è®­ç»ƒä¸€ä¸ªepoch
        avg_loss, current_global_step = train_epoch(
            epoch, 
            start_step if epoch == start_epoch else 0,
            model, train_loader, optimizer, scaler,
            scheduler_fn, ctx, wandb, start_epoch, current_global_step
        )        
        Logger(f"ğŸ“ˆ Epoch {epoch+1}/{CONFIG['epochs']} - å¹³å‡æŸå¤±: {avg_loss:.4f}")
        
        # è®°å½•è®­ç»ƒæŸå¤±åˆ°WandB
        if wandb is not None and (not CONFIG['ddp'] or dist.get_rank() == 0):
            wandb.log({"train/epoch_loss": avg_loss, "epoch": epoch})
        
        # ä¿å­˜epochæ£€æŸ¥ç‚¹
        if not CONFIG['ddp'] or dist.get_rank() == 0:
            checkpoint_path = os.path.join(CONFIG['save_dir'], f'checkpoint_epoch_{epoch+1}.pth')
            save_checkpoint(epoch + 1, 0, model, optimizer, scaler, checkpoint_path, current_global_step)
    
    # ä¿å­˜æœ€ç»ˆæ¨¡å‹
    if not CONFIG['ddp'] or dist.get_rank() == 0:
        final_model_path = os.path.join(CONFIG['save_dir'], 'pawlette.pth')
        if isinstance(model, DistributedDataParallel):
            torch.save(model.module.state_dict(), final_model_path)
        else:
            torch.save(model.state_dict(), final_model_path)
        Logger(f"âœ… è®­ç»ƒå®Œæˆï¼æœ€ç»ˆæ¨¡å‹ä¿å­˜è‡³: {final_model_path}")
    
    # æ¸…ç†
    if CONFIG['ddp']:
        dist.destroy_process_group()
    if wandb is not None:
        wandb.finish()
    Logger("ğŸ‰ Pawletteé¢„è®­ç»ƒå®Œæˆï¼")

if __name__ == "__main__":
    main()