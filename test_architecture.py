#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Pawletteçº¯Mamba2æ¶æ„æµ‹è¯•è„šæœ¬
æµ‹è¯•çº¯Mamba2æ¶æ„çš„æ­£ç¡®æ€§
"""

import torch
import sys
from model.model_pawlette import (
    PawletteConfig, 
    PawletteModelLLM, 
    MambaBlock,
    count_parameters
)


def print_separator(title):
    """æ‰“å°åˆ†éš”ç¬¦"""
    print(f"\n{'='*60}")
    print(f"  {title}")
    print(f"{'='*60}\n")


def test_architecture_layout():
    """æµ‹è¯•1ï¼šéªŒè¯æ¶æ„å¸ƒå±€"""
    print_separator("æµ‹è¯•1: æ¶æ„å¸ƒå±€éªŒè¯")
    
    config = PawletteConfig()
    model = PawletteModelLLM(config)
    
    print(f"æ€»å±‚æ•°: {config.num_hidden_layers}")
    print(f"æ¶æ„ç±»å‹: çº¯Mamba2æ¶æ„")
    print(f"\nå±‚ç±»å‹æ£€æŸ¥:")
    
    mamba_count = 0
    errors = []
    
    for i, layer in enumerate(model.model.layers):
        is_mamba = isinstance(layer, MambaBlock)
        
        status = "âœ…" if is_mamba else "âŒ"
        layer_type = "Mamba" if is_mamba else "Unknown"
        print(f"  å±‚{i:2d}: {layer_type:11s} {status}")
        
        if not is_mamba:
            errors.append(f"Layer {i}: expected Mamba, got {type(layer).__name__}")
        else:
            mamba_count += 1
    
    print(f"\nç»Ÿè®¡:")
    print(f"  Mambaå±‚: {mamba_count}")
    print(f"  æ€»å±‚æ•°: {config.num_hidden_layers}")
    
    if errors:
        print(f"\nâŒ å‘ç° {len(errors)} ä¸ªé”™è¯¯:")
        for error in errors:
            print(f"  - {error}")
        return False
    elif mamba_count != config.num_hidden_layers:
        print(f"\nâŒ å±‚æ•°ä¸åŒ¹é…: æœŸæœ›{config.num_hidden_layers}ï¼Œå®é™…{mamba_count}")
        return False
    else:
        print(f"\nâœ… æ¶æ„å¸ƒå±€æ­£ç¡®ï¼æ‰€æœ‰å±‚éƒ½æ˜¯Mamba2å±‚ï¼")
        return True


def test_forward_pass():
    """æµ‹è¯•2ï¼šå‰å‘ä¼ æ’­"""
    print_separator("æµ‹è¯•2: å‰å‘ä¼ æ’­æµ‹è¯•")
    
    config = PawletteConfig()
    model = PawletteModelLLM(config)
    model.eval()
    
    batch_size = 2
    seq_len = 128
    
    print(f"è¾“å…¥å½¢çŠ¶: batch_size={batch_size}, seq_len={seq_len}")
    
    # åˆ›å»ºæµ‹è¯•è¾“å…¥
    input_ids = torch.randint(0, config.vocab_size, (batch_size, seq_len))
    
    try:
        # è®­ç»ƒæ¨¡å¼ï¼ˆä¸ä½¿ç”¨ç¼“å­˜ï¼‰
        print("\nè®­ç»ƒæ¨¡å¼ï¼ˆuse_cache=Falseï¼‰:")
        model.train()
        with torch.no_grad():
            outputs_train = model(input_ids, use_cache=False)
        
        print(f"  âœ… logitså½¢çŠ¶: {outputs_train.logits.shape}")
        print(f"  âœ… past_key_values: {outputs_train.past_key_values}")
        
        # æ¨ç†æ¨¡å¼ï¼ˆä½¿ç”¨ç¼“å­˜ï¼‰
        print("\næ¨ç†æ¨¡å¼ï¼ˆuse_cache=Trueï¼‰:")
        model.eval()
        with torch.no_grad():
            outputs_infer = model(input_ids, use_cache=True)
        
        print(f"  âœ… logitså½¢çŠ¶: {outputs_infer.logits.shape}")
        print(f"  âœ… past_key_valuesç±»å‹: {type(outputs_infer.past_key_values)}")
        
        if outputs_infer.past_key_values is not None:
            from mamba_ssm.utils.generation import InferenceParams
            is_inference_params = isinstance(outputs_infer.past_key_values, InferenceParams)
            print(f"  âœ… inference_params: {is_inference_params}")
            if is_inference_params:
                print(f"  âœ… Mamba2 ç¼“å­˜å·²æ­£ç¡®åˆå§‹åŒ–")
        
        print(f"\nâœ… å‰å‘ä¼ æ’­æµ‹è¯•é€šè¿‡!")
        return True
        
    except Exception as e:
        print(f"\nâŒ å‰å‘ä¼ æ’­å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


def test_kv_cache():
    """æµ‹è¯•3ï¼šMamba2ç¼“å­˜æœºåˆ¶"""
    print_separator("æµ‹è¯•3: Mamba2ç¼“å­˜æœºåˆ¶æµ‹è¯•")
    
    config = PawletteConfig()
    model = PawletteModelLLM(config)
    model.eval()
    
    # ç¬¬ä¸€æ¬¡è°ƒç”¨ï¼šå¤„ç†åˆå§‹åºåˆ—
    seq_len_1 = 10
    input_ids_1 = torch.randint(0, config.vocab_size, (1, seq_len_1))
    
    print(f"ç¬¬ä¸€æ¬¡è°ƒç”¨: seq_len={seq_len_1}")
    
    try:
        with torch.no_grad():
            outputs_1 = model(input_ids_1, use_cache=True)
        
        print(f"  âœ… è¾“å‡ºlogitså½¢çŠ¶: {outputs_1.logits.shape}")
        
        # éªŒè¯Mamba2ç¼“å­˜
        from mamba_ssm.utils.generation import InferenceParams
        if outputs_1.past_key_values is not None:
            is_inference_params = isinstance(outputs_1.past_key_values, InferenceParams)
            print(f"  âœ… Mamba2 ç¼“å­˜ç±»å‹æ­£ç¡®: {is_inference_params}")
            if is_inference_params:
                print(f"  âœ… seqlen_offset: {outputs_1.past_key_values.seqlen_offset}")
        
        # ç¬¬äºŒæ¬¡è°ƒç”¨ï¼šåªå¤„ç†1ä¸ªæ–°tokenï¼Œä½¿ç”¨ä¹‹å‰çš„ç¼“å­˜
        seq_len_2 = 1
        input_ids_2 = torch.randint(0, config.vocab_size, (1, seq_len_2))
        
        print(f"\nç¬¬äºŒæ¬¡è°ƒç”¨: seq_len={seq_len_2}ï¼ˆä½¿ç”¨ç¼“å­˜ï¼‰")
        
        with torch.no_grad():
            outputs_2 = model(
                input_ids_2,
                past_key_values=outputs_1.past_key_values,
                use_cache=True
            )
        
        print(f"  âœ… è¾“å‡ºlogitså½¢çŠ¶: {outputs_2.logits.shape}")
        
        # éªŒè¯ç¼“å­˜æ›´æ–°
        if outputs_2.past_key_values is not None:
            expected_offset = seq_len_1 + seq_len_2
            actual_offset = outputs_2.past_key_values.seqlen_offset
            print(f"  âœ… seqlen_offset: {actual_offset} (æœŸæœ›: {expected_offset})")
            if actual_offset == expected_offset:
                print(f"  âœ… ç¼“å­˜åç§»é‡æ­£ç¡®!")
            else:
                print(f"  âŒ ç¼“å­˜åç§»é‡ä¸æ­£ç¡®!")
                return False
        
        print(f"\nâœ… Mamba2ç¼“å­˜æœºåˆ¶æµ‹è¯•é€šè¿‡!")
        return True
        
    except Exception as e:
        print(f"\nâŒ Mamba2ç¼“å­˜æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


def test_generation():
    """æµ‹è¯•4ï¼šç”ŸæˆåŠŸèƒ½"""
    print_separator("æµ‹è¯•4: ç”ŸæˆåŠŸèƒ½æµ‹è¯•")
    
    config = PawletteConfig()
    model = PawletteModelLLM(config)
    model.eval()
    
    batch_size = 1
    prompt_len = 10
    max_new_tokens = 20
    
    input_ids = torch.randint(0, config.vocab_size, (batch_size, prompt_len))
    
    print(f"è¾“å…¥é•¿åº¦: {prompt_len}")
    print(f"ç”Ÿæˆé•¿åº¦: {max_new_tokens}")
    
    try:
        # è´ªå©ªè§£ç 
        print("\nè´ªå©ªè§£ç ï¼ˆtemperature=0ï¼‰:")
        with torch.no_grad():
            outputs_greedy = model.generate(
                input_ids,
                max_new_tokens=max_new_tokens,
                do_sample=False,
                pad_token_id=config.pad_token_id,
                eos_token_id=config.eos_token_id,
            )
        
        expected_len = prompt_len + max_new_tokens
        actual_len = outputs_greedy.shape[1]
        
        print(f"  æœŸæœ›é•¿åº¦: {expected_len}")
        print(f"  å®é™…é•¿åº¦: {actual_len}")
        
        if actual_len <= expected_len:
            print(f"  âœ… ç”ŸæˆæˆåŠŸï¼ˆå¯èƒ½æå‰é‡åˆ°EOSï¼‰")
        else:
            print(f"  âŒ ç”Ÿæˆé•¿åº¦è¶…å‡ºé¢„æœŸ")
            return False
        
        # é‡‡æ ·è§£ç 
        print("\né‡‡æ ·è§£ç ï¼ˆtemperature=0.8ï¼‰:")
        with torch.no_grad():
            outputs_sample = model.generate(
                input_ids,
                max_new_tokens=max_new_tokens,
                do_sample=True,
                temperature=0.8,
                top_p=0.9,
                pad_token_id=config.pad_token_id,
                eos_token_id=config.eos_token_id,
            )
        
        actual_len_sample = outputs_sample.shape[1]
        print(f"  æœŸæœ›é•¿åº¦: {expected_len}")
        print(f"  å®é™…é•¿åº¦: {actual_len_sample}")
        
        if actual_len_sample <= expected_len:
            print(f"  âœ… ç”ŸæˆæˆåŠŸï¼ˆå¯èƒ½æå‰é‡åˆ°EOSï¼‰")
        else:
            print(f"  âŒ ç”Ÿæˆé•¿åº¦è¶…å‡ºé¢„æœŸ")
            return False
        
        print(f"\nâœ… ç”ŸæˆåŠŸèƒ½æµ‹è¯•é€šè¿‡!")
        return True
        
    except Exception as e:
        print(f"\nâŒ ç”Ÿæˆæµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


def test_attention_mask():
    """æµ‹è¯•5ï¼šæ³¨æ„åŠ›maskå¤„ç†"""
    print_separator("æµ‹è¯•5: æ³¨æ„åŠ›Maskæµ‹è¯•")
    
    config = PawletteConfig()
    model = PawletteModelLLM(config)
    model.eval()
    
    # åˆ›å»ºæœ‰paddingçš„è¾“å…¥
    pad_token_id = config.pad_token_id
    
    input_ids = torch.tensor([
        [1, 2, 3, 4, 5, pad_token_id, pad_token_id, pad_token_id],  # æœ‰padding
        [1, 2, 3, 4, 5, 6, 7, 8],  # æ— padding
    ])
    
    attention_mask = (input_ids != pad_token_id).long()
    
    print(f"è¾“å…¥å½¢çŠ¶: {input_ids.shape}")
    print(f"ç¬¬1ä¸ªæ ·æœ¬æœ‰æ•ˆé•¿åº¦: {attention_mask[0].sum().item()}")
    print(f"ç¬¬2ä¸ªæ ·æœ¬æœ‰æ•ˆé•¿åº¦: {attention_mask[1].sum().item()}")
    
    try:
        with torch.no_grad():
            outputs = model(input_ids, attention_mask=attention_mask)
        
        print(f"\nâœ… è¾“å‡ºlogitså½¢çŠ¶: {outputs.logits.shape}")
        print(f"âœ… æ³¨æ„åŠ›maskæµ‹è¯•é€šè¿‡!")
        return True
        
    except Exception as e:
        print(f"\nâŒ æ³¨æ„åŠ›maskæµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


def test_parameter_count():
    """æµ‹è¯•6ï¼šå‚æ•°é‡ç»Ÿè®¡"""
    print_separator("æµ‹è¯•6: å‚æ•°é‡ç»Ÿè®¡")
    
    config = PawletteConfig()
    model = PawletteModelLLM(config)
    
    params = count_parameters(model)
    
    print(f"æ€»å‚æ•°é‡: {params['total_M']:.2f}M")
    print(f"å¯è®­ç»ƒå‚æ•°: {params['trainable_M']:.2f}M")
    
    # åˆ†å±‚ç»Ÿè®¡
    print(f"\nåˆ†å±‚å‚æ•°é‡ç»Ÿè®¡:")
    
    # åµŒå…¥å±‚
    embed_params = sum(p.numel() for p in model.model.embed_tokens.parameters())
    print(f"  åµŒå…¥å±‚: {embed_params/1e6:.2f}M")
    
    # Mambaå±‚
    mamba_params = 0
    
    for i, layer in enumerate(model.model.layers):
        layer_params = sum(p.numel() for p in layer.parameters())
        mamba_params += layer_params
        if i == 0:  # åªæ‰“å°ç¬¬ä¸€ä¸ªMambaå±‚ä½œä¸ºç¤ºä¾‹
            print(f"  å±‚{i:2d} (Mamba): {layer_params/1e6:.2f}M")
    
    print(f"  ... (çœç•¥å…¶ä»–Mambaå±‚)")
    print(f"\n  æ€»Mambaå‚æ•°: {mamba_params/1e6:.2f}M")
    
    # è¾“å‡ºå±‚
    output_params = sum(p.numel() for p in model.lm_head.parameters())
    print(f"  è¾“å‡ºå±‚: {output_params/1e6:.2f}M")
    
    # å½’ä¸€åŒ–å±‚
    norm_params = sum(p.numel() for p in model.model.norm.parameters())
    print(f"  å½’ä¸€åŒ–å±‚: {norm_params/1e3:.2f}K")
    
    print(f"\nâœ… å‚æ•°é‡ç»Ÿè®¡å®Œæˆ!")
    return True


def run_all_tests():
    """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
    print("\n" + "="*60)
    print("  Pawletteçº¯Mamba2æ¶æ„æµ‹è¯•å¥—ä»¶")
    print("="*60)
    
    tests = [
        ("æ¶æ„å¸ƒå±€éªŒè¯", test_architecture_layout),
        ("å‰å‘ä¼ æ’­æµ‹è¯•", test_forward_pass),
        ("KVç¼“å­˜æœºåˆ¶æµ‹è¯•", test_kv_cache),
        ("ç”ŸæˆåŠŸèƒ½æµ‹è¯•", test_generation),
        ("æ³¨æ„åŠ›Maskæµ‹è¯•", test_attention_mask),
        ("å‚æ•°é‡ç»Ÿè®¡", test_parameter_count),
    ]
    
    results = []
    
    for test_name, test_func in tests:
        try:
            success = test_func()
            results.append((test_name, success))
        except Exception as e:
            print(f"\nâŒ æµ‹è¯• '{test_name}' å‘ç”Ÿå¼‚å¸¸: {e}")
            import traceback
            traceback.print_exc()
            results.append((test_name, False))
    
    # æ‰“å°æ€»ç»“
    print_separator("æµ‹è¯•æ€»ç»“")
    
    passed = 0
    failed = 0
    
    for test_name, success in results:
        status = "âœ… é€šè¿‡" if success else "âŒ å¤±è´¥"
        print(f"{status} - {test_name}")
        if success:
            passed += 1
        else:
            failed += 1
    
    print(f"\næ€»è®¡: {passed} é€šè¿‡, {failed} å¤±è´¥")
    
    if failed == 0:
        print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼çº¯Mamba2æ¶æ„è¿è¡Œæ­£å¸¸ã€‚")
        return True
    else:
        print(f"\nâš ï¸ æœ‰ {failed} ä¸ªæµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥ä»£ç ã€‚")
        return False


if __name__ == "__main__":
    success = run_all_tests()
    sys.exit(0 if success else 1)


