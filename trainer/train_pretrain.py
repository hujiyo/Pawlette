import os
import sys
__package__ = "trainer"
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

import time
import math
import warnings
import torch
import torch.distributed as dist
from torch import optim, nn
from torch.nn.parallel import DistributedDataParallel
from torch.utils.data import DataLoader, DistributedSampler
from contextlib import nullcontext
from transformers import AutoTokenizer
from model.model_pawlette import PawletteConfig, PawletteForCausalLM, count_parameters
from dataset.lm_dataset import PretrainDataset, dynamic_collate_fn

warnings.filterwarnings('ignore')

# å…¨å±€é…ç½® - åªåŒ…å«è®­ç»ƒç›¸å…³å‚æ•°
CONFIG = {
    
    # è®­ç»ƒé…ç½®
    'epochs': 1,
    'batch_size': 16,  # å‡å°æ‰¹æ¬¡å¤§å°ä»¥é™ä½å†…å­˜å ç”¨
    'learning_rate': 5e-4,
    'warmup_steps': 100, #æŒ‡é¢„çƒ­æ­¥æ•°
    'accumulation_steps': 8,  # å¢åŠ æ¢¯åº¦ç´¯ç§¯æ­¥æ•°ä»¥ä¿æŒæœ‰æ•ˆæ‰¹æ¬¡å¤§å°
    'grad_clip': 1.0,
    'weight_decay': 0.01,
    
    # æ•°æ®é…ç½®
    'data_path': '../dataset/pretrain_data.jsonl',
    'eval_data_path': None,
    'max_seq_len': None,  # ä¸é™åˆ¶åºåˆ—é•¿åº¦
    'tokenizer_path': '../model/',
    
    # è¾“å‡ºé…ç½®
    'out_dir': '../out',
    'log_interval': 100,
    'save_interval': 500,
    'eval_interval': 500,
    
    # ç»§ç»­è®­ç»ƒé…ç½®
    'continue_pretrain': False,
    'pretrained_path': None,
    'resume': False,
    'checkpoint_path': None,
    
    # åˆ†å¸ƒå¼è®­ç»ƒ
    'ddp': False,
    'num_workers': 4,
    
    # å…¶ä»–é…ç½®
    'device': 'cuda:0' if torch.cuda.is_available() else 'cpu',
    'dtype': 'bfloat16',
    'seed': 42,
    'use_wandb': False,
    'wandb_project': 'Pawlette-Pretrain',
    
    # å†…å­˜ä¼˜åŒ–é…ç½®
    'gradient_checkpointing': True,  # å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹ä»¥èŠ‚çœå†…å­˜
}

# å…¨å±€å˜é‡
ddp = CONFIG['ddp']


def Logger(content):
    """ç»Ÿä¸€çš„æ—¥å¿—è¾“å‡ºå‡½æ•°"""
    try:
        if not ddp or dist.get_rank() == 0:
            print(f"[Pawlette] {content}")
    except NameError:
        print(f"[Pawlette] {content}")


def get_cosine_schedule_with_warmup(current_step, num_warmup_steps, num_training_steps, min_lr_ratio=0.1):
    """ä½™å¼¦é€€ç«å­¦ä¹ ç‡è°ƒåº¦ï¼ˆå¸¦warmupï¼‰"""
    if current_step < num_warmup_steps:
        # Warmupé˜¶æ®µ
        return float(current_step) / float(max(1, num_warmup_steps))
    
    # ä½™å¼¦é€€ç«é˜¶æ®µ
    progress = float(current_step - num_warmup_steps) / float(max(1, num_training_steps - num_warmup_steps))
    lr_mult = min_lr_ratio + (1 - min_lr_ratio) * 0.5 * (1.0 + math.cos(math.pi * progress))
    return lr_mult


def save_checkpoint(epoch, step, model, optimizer, scaler, best_loss, save_path):
    """ä¿å­˜æ£€æŸ¥ç‚¹"""
    state = {
        'epoch': epoch,
        'step': step,
        'model_state_dict': model.module.state_dict() if isinstance(model, DistributedDataParallel) else model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'scaler_state_dict': scaler.state_dict(),
        'best_loss': best_loss,
        'config': model.module.config if isinstance(model, DistributedDataParallel) else model.config,
    }
    torch.save(state, save_path)
    Logger(f"âœ… å·²ä¿å­˜æ£€æŸ¥ç‚¹è‡³ {save_path}")


def load_checkpoint(model, optimizer, scaler, checkpoint_path, device, strict=True):
    """åŠ è½½æ£€æŸ¥ç‚¹"""
    # æ·»åŠ å®‰å…¨çš„å…¨å±€ç±»ä»¥æ”¯æŒPyTorch 2.6çš„weights_onlyæ¨¡å¼
    from model.model_pawlette import PawletteConfig

    # å¦‚æœ PyTorch ç‰ˆæœ¬ >= 2.6ï¼Œåˆ™æ·»åŠ å®‰å…¨å…¨å±€ç±»
    if hasattr(torch.serialization, 'add_safe_globals'):
        torch.serialization.add_safe_globals([PawletteConfig])
    
    checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=False)
    
    # åŠ è½½æ¨¡å‹çŠ¶æ€
    if isinstance(model, DistributedDataParallel):
        model.module.load_state_dict(checkpoint['model_state_dict'], strict=strict)
    else:
        model.load_state_dict(checkpoint['model_state_dict'], strict=strict)
    
    # åŠ è½½ä¼˜åŒ–å™¨å’ŒscalerçŠ¶æ€
    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
    scaler.load_state_dict(checkpoint['scaler_state_dict'])
    
    start_epoch = checkpoint.get('epoch', 0)
    start_step = checkpoint.get('step', 0)
    best_loss = checkpoint.get('best_loss', float('inf'))
    
    Logger(f"âœ… å·²ä» {checkpoint_path} åŠ è½½æ£€æŸ¥ç‚¹")
    Logger(f"   ç»§ç»­è®­ç»ƒ: epoch={start_epoch}, step={start_step}, best_loss={best_loss:.4f}")
    
    return start_epoch, start_step, best_loss


def evaluate_model(model, eval_loader, ctx, device):
    """è¯„ä¼°æ¨¡å‹"""
    model.eval()
    total_loss = 0
    total_tokens = 0
    
    with torch.no_grad():
        for X, Y, loss_mask in eval_loader:
            X = X.to(device)
            Y = Y.to(device)
            loss_mask = loss_mask.to(device)
            
            with ctx:
                outputs = model(input_ids=X, labels=Y)
                
                # ä½¿ç”¨maskè®¡ç®—æŸå¤±
                if loss_mask is not None:
                    loss_values = nn.functional.cross_entropy(
                        outputs.logits.view(-1, outputs.logits.size(-1)),
                        Y.view(-1),
                        reduction='none'
                    ).view(Y.size())
                    loss = (loss_values * loss_mask).sum()
                    num_tokens = loss_mask.sum()
                else:
                    loss = outputs.loss * X.size(0) * X.size(1)
                    num_tokens = X.size(0) * X.size(1)
                
                total_loss += loss.item()
                total_tokens += num_tokens.item()
    
    avg_loss = total_loss / total_tokens if total_tokens > 0 else float('inf')
    model.train()
    return avg_loss


def train_epoch(epoch, start_step, model, train_loader, optimizer, scaler, 
                scheduler_fn, ctx, wandb=None, start_epoch=0):
    """è®­ç»ƒä¸€ä¸ªepoch"""
    model.train()
    loss_fct = nn.CrossEntropyLoss(reduction='none')
    
    total_loss = 0
    total_tokens = 0
    start_time = time.time()
    first_step = True  # æ ‡è®°æ˜¯å¦æ˜¯ç¬¬ä¸€ä¸ªå®é™…æ‰§è¡Œçš„æ­¥éª¤
    executed_steps = 0  # å®é™…æ‰§è¡Œçš„æ­¥æ•°è®¡æ•°å™¨
    
    for step, (X, Y, loss_mask) in enumerate(train_loader):
        # è·³è¿‡å·²è®­ç»ƒçš„æ­¥éª¤ï¼ˆç”¨äºæ–­ç‚¹ç»­è®­ï¼‰
        if epoch == start_epoch and step < start_step:
            continue
        
        # å¦‚æœæ˜¯ç¬¬ä¸€ä¸ªå®é™…æ‰§è¡Œçš„æ­¥éª¤ï¼Œé‡æ–°è®¾ç½®å¼€å§‹æ—¶é—´
        if first_step:
            start_time = time.time()
            first_step = False
        
        # å¢åŠ å®é™…æ‰§è¡Œçš„æ­¥æ•°è®¡æ•°
        executed_steps += 1
        
        X = X.to(CONFIG['device'])
        Y = Y.to(CONFIG['device'])
        loss_mask = loss_mask.to(CONFIG['device'])
        
        # è®¡ç®—å½“å‰æ­¥çš„å…¨å±€æ­¥æ•°ï¼ˆè€ƒè™‘æ–­ç‚¹ç»­è®­ï¼‰
        if epoch == start_epoch:
            global_step = step
        else:
            global_step = epoch * len(train_loader) + step
        
        # æ›´æ–°å­¦ä¹ ç‡
        lr_mult = scheduler_fn(global_step)
        for param_group in optimizer.param_groups:
            param_group['lr'] = CONFIG['learning_rate'] * lr_mult
        
        # å‰å‘ä¼ æ’­
        with ctx:
            outputs = model(input_ids=X, labels=Y)
            
            # ä½¿ç”¨maskè®¡ç®—æŸå¤±
            if loss_mask is not None:
                loss_values = loss_fct(
                    outputs.logits.view(-1, outputs.logits.size(-1)),
                    Y.view(-1)
                ).view(Y.size())
                loss = (loss_values * loss_mask).sum() / loss_mask.sum()
            else:
                loss = outputs.loss
            
            # æ¢¯åº¦ç´¯ç§¯
            loss = loss / CONFIG['accumulation_steps']
        
        # åå‘ä¼ æ’­
        scaler.scale(loss).backward()
        
        # æ¢¯åº¦ç´¯ç§¯æ­¥éª¤
        if (step + 1) % CONFIG['accumulation_steps'] == 0:
            # æ¢¯åº¦è£å‰ª
            scaler.unscale_(optimizer)
            torch.nn.utils.clip_grad_norm_(model.parameters(), CONFIG['grad_clip'])
            
            # ä¼˜åŒ–å™¨æ­¥éª¤
            scaler.step(optimizer)
            scaler.update()
            optimizer.zero_grad(set_to_none=True)
        
        # ç»Ÿè®¡
        total_loss += loss.item() * CONFIG['accumulation_steps']
        if loss_mask is not None:
            total_tokens += loss_mask.sum().item()
        else:
            total_tokens += X.numel()
        
        # æ—¥å¿—è¾“å‡º
        if step % CONFIG['log_interval'] == 0:
            elapsed_time = time.time() - start_time
            
            if executed_steps > 0:
                avg_time_per_step = elapsed_time / executed_steps
                # è®¡ç®—å‰©ä½™æ­¥æ•°
                remaining_steps = len(train_loader) - step - 1
                remaining_time = avg_time_per_step * remaining_steps / 60
            else:
                remaining_time = 0
            
            current_loss = loss.item() * CONFIG['accumulation_steps']
            current_lr = optimizer.param_groups[0]['lr']
            tokens_per_sec = total_tokens / elapsed_time if elapsed_time > 0 else 0
            
            Logger(
                f'Epoch:[{epoch+1}/{CONFIG["epochs"]}]({step}/{len(train_loader)}) '
                f'loss:{current_loss:.4f} lr:{current_lr:.2e} '
                f'tokens/s:{tokens_per_sec:.0f} eta:{remaining_time:.1f}min'
            )
            
            # WandBæ—¥å¿—
            if wandb is not None and (not ddp or dist.get_rank() == 0):
                wandb.log({
                    "train/loss": current_loss,
                    "train/lr": current_lr,
                    "train/tokens_per_sec": tokens_per_sec,
                    "train/global_step": global_step,
                })
        
        # å®šæœŸä¿å­˜æ£€æŸ¥ç‚¹
        if (step + 1) % CONFIG['save_interval'] == 0 and (not ddp or dist.get_rank() == 0):
            checkpoint_path = os.path.join(CONFIG['save_dir'], 'checkpoint_latest.pth')
            save_checkpoint(epoch, step + 1, model, optimizer, scaler, total_loss / (step + 1), checkpoint_path)
            
            # ä¿å­˜æ¨¡å‹æƒé‡
            model_path = os.path.join(CONFIG['save_dir'], 'pawlette.pth')
            if isinstance(model, DistributedDataParallel):
                torch.save(model.module.state_dict(), model_path)
            else:
                torch.save(model.state_dict(), model_path)
            Logger(f"âœ… å·²ä¿å­˜æ¨¡å‹æƒé‡è‡³ {model_path}")
    
    # ä½¿ç”¨å®é™…æ‰§è¡Œçš„æ­¥æ•°è®¡ç®—å¹³å‡æŸå¤±
    avg_loss = total_loss / max(1, executed_steps)
    return avg_loss


def init_model():
    """åˆå§‹åŒ–æ¨¡å‹å’Œåˆ†è¯å™¨"""
    
    # ç›´æ¥ä½¿ç”¨PawletteConfigçš„é»˜è®¤é…ç½®
    config = PawletteConfig()
    
    Logger(f"ğŸ“ æ¨¡å‹é…ç½®:")
    Logger(f"   - hidden_size: {config.hidden_size}")
    Logger(f"   - num_layers: {config.num_hidden_layers}")
    Logger(f"   - state_size: {config.state_size}")
    Logger(f"   - vocab_size: {config.vocab_size}")
    
    # åˆ›å»ºæ¨¡å‹
    model = PawletteForCausalLM(config)
    
    # åŠ è½½é¢„è®­ç»ƒæƒé‡ï¼ˆå¦‚æœæŒ‡å®šï¼‰
    if CONFIG['continue_pretrain'] and CONFIG['pretrained_path']:
        if os.path.exists(CONFIG['pretrained_path']):
            Logger(f"ğŸ“‚ åŠ è½½é¢„è®­ç»ƒæ¨¡å‹: {CONFIG['pretrained_path']}")
            state_dict = torch.load(CONFIG['pretrained_path'], map_location=CONFIG['device'])
            model.load_state_dict(state_dict, strict=False)
            Logger("âœ… é¢„è®­ç»ƒæ¨¡å‹åŠ è½½æˆåŠŸ")
        else:
            Logger(f"âš ï¸ é¢„è®­ç»ƒæ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {CONFIG['pretrained_path']}")
    
    # åŠ è½½åˆ†è¯å™¨
    if os.path.exists(CONFIG['tokenizer_path']):
        tokenizer = AutoTokenizer.from_pretrained(CONFIG['tokenizer_path'], trust_remote_code=True)
    else:
        Logger(f"âš ï¸ ä½¿ç”¨é»˜è®¤åˆ†è¯å™¨ï¼Œæœªæ‰¾åˆ°: {CONFIG['tokenizer_path']}")
        from transformers import GPT2Tokenizer
        tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
        tokenizer.pad_token = tokenizer.eos_token
    
    # ç§»åŠ¨æ¨¡å‹åˆ°è®¾å¤‡
    model = model.to(CONFIG['device'])
    
    # å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹ä»¥èŠ‚çœå†…å­˜
    if CONFIG['gradient_checkpointing']:
        model.gradient_checkpointing_enable()
        Logger("âœ… å·²å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹ä»¥ä¼˜åŒ–å†…å­˜ä½¿ç”¨")
    
    # ç»Ÿè®¡å‚æ•°é‡
    params = count_parameters(model)
    Logger(f"ğŸ“Š æ¨¡å‹å‚æ•°é‡: {params['trainable_M']:.2f}M (å¯è®­ç»ƒ) / {params['total_M']:.2f}M (æ€»è®¡)")
    
    return model, tokenizer, config


def init_distributed_mode():
    """åˆå§‹åŒ–åˆ†å¸ƒå¼è®­ç»ƒ"""
    if not CONFIG['ddp']:
        return
    
    dist.init_process_group(backend="nccl")
    CONFIG['ddp_rank'] = int(os.environ["RANK"])
    CONFIG['ddp_local_rank'] = int(os.environ["LOCAL_RANK"])
    CONFIG['ddp_world_size'] = int(os.environ["WORLD_SIZE"])
    CONFIG['device'] = f"cuda:{CONFIG['ddp_local_rank']}"
    torch.cuda.set_device(CONFIG['device'])
    
    Logger(f"ğŸŒ åˆ†å¸ƒå¼è®­ç»ƒåˆå§‹åŒ–: rank={CONFIG['ddp_rank']}, world_size={CONFIG['ddp_world_size']}")


def main():
    """ä¸»è®­ç»ƒå‡½æ•°"""
    global ddp
    
    # è®¾ç½®éšæœºç§å­
    torch.manual_seed(CONFIG['seed'])
    if torch.cuda.is_available():
        torch.cuda.manual_seed(CONFIG['seed'])
    
    # è®¾ç½®å…¨å±€å˜é‡
    ddp = CONFIG['ddp']
    
    # åˆå§‹åŒ–åˆ†å¸ƒå¼è®­ç»ƒ
    if CONFIG['ddp']:
        init_distributed_mode()
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    CONFIG['save_dir'] = os.path.join(CONFIG['out_dir'], "pawlette")
    os.makedirs(CONFIG['save_dir'], exist_ok=True)
    
    # åˆå§‹åŒ–WandB
    wandb = None
    if CONFIG['use_wandb'] and (not CONFIG['ddp'] or dist.get_rank() == 0):
        import wandb
        run_name = f"Pawlette-bs{CONFIG['batch_size']}-lr{CONFIG['learning_rate']}"
        wandb.init(project=CONFIG['wandb_project'], name=run_name, config=CONFIG)
    
    Logger("ğŸ¾ Pawletteé¢„è®­ç»ƒå¼€å§‹")
    Logger(f"ğŸ“ è¾“å‡ºç›®å½•: {CONFIG['save_dir']}")
    
    # è®¾ç½®è®¾å¤‡å’Œæ··åˆç²¾åº¦
    device_type = "cuda" if "cuda" in CONFIG['device'] else "cpu"
    dtype_map = {
        "float32": torch.float32,
        "float16": torch.float16,
        "bfloat16": torch.bfloat16,
    }
    torch_dtype = dtype_map[CONFIG['dtype']]
    ctx = nullcontext() if device_type == "cpu" else torch.cuda.amp.autocast(dtype=torch_dtype)
    
    # åˆå§‹åŒ–æ¨¡å‹å’Œåˆ†è¯å™¨
    model, tokenizer, config = init_model()
    
    
    # å‡†å¤‡æ•°æ®é›†
    train_dataset = PretrainDataset(CONFIG['data_path'], tokenizer, max_length=CONFIG['max_seq_len'])
    Logger(f"ğŸ“š è®­ç»ƒæ•°æ®é›†å¤§å°: {len(train_dataset)}")
    if CONFIG['max_seq_len'] is None:
        Logger("ğŸ“ åºåˆ—é•¿åº¦: æ— é™åˆ¶ï¼ˆåŠ¨æ€é•¿åº¦ï¼‰")
    else:
        Logger(f"ğŸ“ åºåˆ—é•¿åº¦: {CONFIG['max_seq_len']}")
    
    # æ•°æ®åŠ è½½å™¨
    train_sampler = DistributedSampler(train_dataset) if CONFIG['ddp'] else None
    train_loader = DataLoader(
        train_dataset,
        batch_size=CONFIG['batch_size'],
        shuffle=(train_sampler is None),
        sampler=train_sampler,
        num_workers=CONFIG['num_workers'],
        pin_memory=True,
        drop_last=True,
        collate_fn=dynamic_collate_fn if CONFIG['max_seq_len'] is None else None,
    )
    
    # éªŒè¯æ•°æ®é›†ï¼ˆå¦‚æœæœ‰ï¼‰
    eval_loader = None
    if CONFIG['eval_data_path'] and os.path.exists(CONFIG['eval_data_path']):
        eval_dataset = PretrainDataset(CONFIG['eval_data_path'], tokenizer, max_length=CONFIG['max_seq_len'])
        eval_loader = DataLoader(
            eval_dataset,
            batch_size=CONFIG['batch_size'],
            shuffle=False,
            num_workers=CONFIG['num_workers'],
            pin_memory=True,
            collate_fn=dynamic_collate_fn if CONFIG['max_seq_len'] is None else None,
        )
        Logger(f"ğŸ“š éªŒè¯æ•°æ®é›†å¤§å°: {len(eval_dataset)}")
    
    # ä¼˜åŒ–å™¨
    optimizer = optim.AdamW(
        model.parameters(),
        lr=CONFIG['learning_rate'],
        weight_decay=CONFIG['weight_decay'],
        betas=(0.9, 0.95),
    )
    
    # æ··åˆç²¾åº¦è®­ç»ƒ
    scaler = torch.cuda.amp.GradScaler(enabled=(CONFIG['dtype'] in ['float16', 'bfloat16']))
    
    # å­¦ä¹ ç‡è°ƒåº¦å™¨
    total_steps = len(train_loader) * CONFIG['epochs']
    scheduler_fn = lambda step: get_cosine_schedule_with_warmup(
        step, CONFIG['warmup_steps'], total_steps, min_lr_ratio=0.1
    )
    
    # æ–­ç‚¹ç»­è®­ - è‡ªåŠ¨æ£€æµ‹æ£€æŸ¥ç‚¹æ–‡ä»¶
    start_epoch, start_step = 0, 0
    best_loss = float('inf')
    
    # è‡ªåŠ¨æ£€æµ‹æ£€æŸ¥ç‚¹æ–‡ä»¶
    checkpoint_path = os.path.join(CONFIG['save_dir'], 'checkpoint_latest.pth')
    if os.path.exists(checkpoint_path):
        Logger(f"ğŸ” è‡ªåŠ¨æ£€æµ‹åˆ°æ£€æŸ¥ç‚¹æ–‡ä»¶: {checkpoint_path}")
        Logger("ğŸ”„ è‡ªåŠ¨å¯ç”¨æ–­ç‚¹ç»­è®­...")
        start_epoch, start_step, best_loss = load_checkpoint(
            model, optimizer, scaler, checkpoint_path, CONFIG['device']
        )
    elif CONFIG['resume'] and CONFIG['checkpoint_path']:
        # å¦‚æœæ‰‹åŠ¨æŒ‡å®šäº†æ£€æŸ¥ç‚¹è·¯å¾„
        checkpoint_path = CONFIG['checkpoint_path']
        if os.path.exists(checkpoint_path):
            start_epoch, start_step, best_loss = load_checkpoint(
                model, optimizer, scaler, checkpoint_path, CONFIG['device']
            )
        else:
            Logger(f"âš ï¸ æœªæ‰¾åˆ°æ£€æŸ¥ç‚¹æ–‡ä»¶: {checkpoint_path}")
    else:
        # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨è®­ç»ƒå¥½çš„æ¨¡å‹æ–‡ä»¶
        existing_models = []
        model_files = [
            'pawlette.pth',
            'pawlette_best.pth', 
            'pawlette_final.pth'
        ]
        
        for model_file in model_files:
            model_path = os.path.join(CONFIG['save_dir'], model_file)
            if os.path.exists(model_path):
                existing_models.append(model_file)
        
        if existing_models:
            Logger("âš ï¸ æ£€æµ‹åˆ°å·²å­˜åœ¨çš„æ¨¡å‹æ–‡ä»¶:")
            for model_file in existing_models:
                Logger(f"   - {model_file}")
            Logger("ğŸ›‘ ä¸ºé˜²æ­¢è¦†ç›–å·²è®­ç»ƒçš„æ¨¡å‹ï¼Œè®­ç»ƒå·²åœæ­¢ï¼")
            Logger("ğŸ’¡ å¦‚éœ€ç»§ç»­è®­ç»ƒï¼Œè¯·:")
            Logger("   1. åˆ é™¤æˆ–é‡å‘½åç°æœ‰æ¨¡å‹æ–‡ä»¶")
            Logger("   2. æˆ–è€…ä½¿ç”¨ --resume å‚æ•°æŒ‡å®šæ£€æŸ¥ç‚¹æ–‡ä»¶")
            Logger("   3. æˆ–è€…ä¿®æ”¹è¾“å‡ºç›®å½• --out_dir")
            return
        else:
            Logger("ğŸ†• æœªæ£€æµ‹åˆ°æ£€æŸ¥ç‚¹æ–‡ä»¶å’Œæ¨¡å‹æ–‡ä»¶ï¼Œå¼€å§‹å…¨æ–°è®­ç»ƒ...")
    
    # DDPåŒ…è£…
    if CONFIG['ddp']:
        model = DistributedDataParallel(model, device_ids=[CONFIG['ddp_local_rank']])
    
    # è®­ç»ƒå¾ªç¯
    Logger("ğŸš€ å¼€å§‹è®­ç»ƒ...")
    for epoch in range(start_epoch, CONFIG['epochs']):
        if CONFIG['ddp'] and train_sampler is not None:
            train_sampler.set_epoch(epoch)
        
        # è®­ç»ƒä¸€ä¸ªepoch
        avg_loss = train_epoch(
            epoch, 
            start_step if epoch == start_epoch else 0,
            model, train_loader, optimizer, scaler,
            scheduler_fn, ctx, wandb, start_epoch
        )
        
        Logger(f"ğŸ“ˆ Epoch {epoch+1}/{CONFIG['epochs']} - å¹³å‡æŸå¤±: {avg_loss:.4f}")
        
        # è¯„ä¼°ï¼ˆå¦‚æœæœ‰éªŒè¯é›†ï¼‰
        if eval_loader is not None and (not CONFIG['ddp'] or dist.get_rank() == 0):
            eval_loss = evaluate_model(model, eval_loader, ctx, CONFIG['device'])
            Logger(f"ğŸ“Š éªŒè¯æŸå¤±: {eval_loss:.4f}")
            
            if wandb is not None:
                wandb.log({"eval/loss": eval_loss, "epoch": epoch})
            
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if eval_loss < best_loss:
                best_loss = eval_loss
                best_model_path = os.path.join(CONFIG['save_dir'], 'pawlette_best.pth')
                if isinstance(model, DistributedDataParallel):
                    torch.save(model.module.state_dict(), best_model_path)
                else:
                    torch.save(model.state_dict(), best_model_path)
                Logger(f"ğŸ† ä¿å­˜æœ€ä½³æ¨¡å‹ (loss={best_loss:.4f})")
        
        # ä¿å­˜epochæ£€æŸ¥ç‚¹
        if not CONFIG['ddp'] or dist.get_rank() == 0:
            checkpoint_path = os.path.join(CONFIG['save_dir'], f'checkpoint_epoch_{epoch+1}.pth')
            save_checkpoint(epoch + 1, 0, model, optimizer, scaler, best_loss, checkpoint_path)
    
    # ä¿å­˜æœ€ç»ˆæ¨¡å‹
    if not CONFIG['ddp'] or dist.get_rank() == 0:
        final_model_path = os.path.join(CONFIG['save_dir'], 'pawlette_final.pth')
        if isinstance(model, DistributedDataParallel):
            torch.save(model.module.state_dict(), final_model_path)
        else:
            torch.save(model.state_dict(), final_model_path)
        Logger(f"âœ… è®­ç»ƒå®Œæˆï¼æœ€ç»ˆæ¨¡å‹ä¿å­˜è‡³: {final_model_path}")
    
    # æ¸…ç†
    if CONFIG['ddp']:
        dist.destroy_process_group()
    
    if wandb is not None:
        wandb.finish()
    
    Logger("ğŸ‰ Pawletteé¢„è®­ç»ƒå®Œæˆï¼")


if __name__ == "__main__":
    main()